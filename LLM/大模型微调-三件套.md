# 一 Prompt
## 1 Prefix-Tuning思想： 
+ 在每一层token之前构造一段任务相关的tokens作为Prefix，训练时只更新Prefix部分的参数，而Transformer中其他部分参数固定 
+ 一个Prefix模块就是一个可学习的id到embedding映射表，可在多层分别添加Prefix模块 
+ 止直接更新Prefix的参数导致训练不稳定的情况，在Prefix层前面增加MLP结构 
+ 作用阶段:所有Transformer block的attention注意力计算

![Prefix-Tuning思想](images/Prefix示例.png)

## 2 Prompt-Tuning思想： 
+ 可视为Prefix-Tuning的简化版，只在输入层加入prompt token，并不需要加入MLP进行调整 
+ 提出prompt Ensembling方法来集成预训练语言模型的多种prompts 
+ 只额外增加3.6%参数规模的情况下取得和全微调接近的效果 
+ 作用阶段：第一层transformer block的注意力计算
	
![Prompt-Tuning思想](images/20230825005640.png)

## 3 虚拟token只做K V，不做Q
## 4 大模型的prefixtuing的微调都是用transformers提供的peft库，但是清华的chatglm系列，采用了特殊的结构，和自由的命名规范 导致 可以用lora，但是不能用prefix-tuning
## 5. 代码
finetune_prompt.py
--pre_sql_len 128:给原生的每层加上128的软token
modeling_chatglm.py
![[modeling_chatglm 2.py]]

![](images/20230825011709.png)
![](images/20230825011912.png)(https://zhuanlan.zhihu.com/p/635686756)
[大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/635848732)
# 二 适配器
## 1.结构
![](images/20230825013428.png)
为一resnet结构，微调时只微调adapter

## 2.chatglm,peft都没有实现
## 3.代码：finetune_adapter.py
![](code/finetune_adapter.py)
示例：
![](imaegs/20230825015355.png)
![](images/20230825014843.png)
	
## 4.适配器需单独保存，加载
保存：
![](images/20230825015817.png)
	加载：（手动改结构拼接）
![](images/20230825020024.png)

# 三 Lora
## 1.结构
![](images/20230826171919.png)

## 2.r:线性代数中矩阵的秩（矩阵实际的维度）
好处：节约内存

## 3.初始化：为了模型一开始保持原状，初始化W0=A B=0，A=0 or B=0,一个正态分布，一个为0，但不能同时为0（梯度为0，参数不更新）
![](images/20230826172606.png)

## 4.参数量：原：h=Wx (h:m* 1,x:n* 1)
lora:W0=A B (m* r+n* r<<mn)
r=4,8,16 领域越垂直，r越大，参数越多，可学习的信息越多；
越泛化通用，r越小；
r过大，对原有模型破坏越严重->产生灾难性遗忘（P-Tuning，Adapter也一样）

## 5.Lora作用于：attention中的Wq,Wk,Wv，
由于attention基本思想是v的加权求和，加权权重由q,k决定，改变权重时，调整两个和调整一个的效果是一样的，因此只需要改变一个矩阵（Wq,Wk二选一）

## 6.计算量并没有减少，但速度确更快，原因：
+ 只更新了部分参数：比如LoRA原论文就选择只更新Self Attention的参数， 实际使用时我们还可以选择只更新部分层的参数； 
+ 减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要 传输的数据量也变少了，从而减少了传输时间； 
+ 采用了各种低精度加速技术，如FP16、FP8或者INT8量化等。

内存占用粗略公式：6x可训练参数+2x不可训练参数

## 7 代码
![[finetune_lora.py]]

 示例：
 ![](images/20230826175249.png)
+ 来源peft：peft.get_peft_model
+ inference_mode:推理模式，是否训练；
+ lora_alpha:W0前面的系数，Lora的权重（W+W0->W+alpha* W0）

# 四 Langchain——人工构造prompt工程
## 1.方法：构造一个知识库 
+ 知识来源：pdf ocr word->纯文本 
+ 知识类型：qa问答对，非结构化纯文本 知识向量化（bert模型）：qa问答对的问题向量化 ，非结构化文本直接向量化


## 2.示例
+ 庐山有什么好玩的
+ 在向量库检索最相似的知识，放在前面做prompt

## 3.代码
![[generate_vector.py]]

![[test_langchain.py]]

## 4.总结：Langchain 利用向量检索，搜寻知识库，拼接prompt 并不会对模型进行改变 
+ 优势：简单，计算资源也少，也不需要训练，只需构建知识库 
+ 缺点：上限取决于模型本身能力。迁移性差，不同模型需要构造不同prompt

## 5.模型输入长度有限，如果知识过长，需要内容压缩。给模型训练不曾见过（信息量高）的知识，效果才好。句子s，模型生成s的概率越低，s带来的信息量越大，信息量-log p。
	
![[compress.py]]

+ 如何生成一句话的概率：
chatglm输入输出：
![](images/20230826222938.png)
+ 思想：让一个空输入输出这句话
![](images/20230826223047.png)
+ 计算输出和目标之间各token的（信息熵）loss，取平均
+ 压缩：在相似句子中，优先选择信息熵（loss）高的句子

## 6.总结微调方法
+ 截止目前Lora 最常用，最好用的方法 
+ Langchain：门槛最低，最容易的方法 
+ Ptuning，直接微调
+ adapter：占用资源相对较多，用的相对较
